{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fake-useragent in c:\\programdata\\anaconda3\\lib\\site-packages (0.1.11)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fake-useragent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: SPARQLWrapper in c:\\programdata\\anaconda3\\lib\\site-packages (1.8.5)\n",
      "Requirement already satisfied: rdflib>=4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from SPARQLWrapper) (4.2.2)\n",
      "Requirement already satisfied: pyparsing in c:\\programdata\\anaconda3\\lib\\site-packages (from rdflib>=4.0->SPARQLWrapper) (2.4.2)\n",
      "Requirement already satisfied: isodate in c:\\programdata\\anaconda3\\lib\\site-packages (from rdflib>=4.0->SPARQLWrapper) (0.6.0)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from isodate->rdflib>=4.0->SPARQLWrapper) (1.12.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install SPARQLWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Директория D:\\Univer\\Сбербанк\\Work\\Geography\\Geography\\Information уже создана\n"
     ]
    }
   ],
   "source": [
    "from fake_useragent import UserAgent\n",
    "from os.path import join,isfile\n",
    "from os import listdir\n",
    "import dask\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import pandas as pd\n",
    "import time\n",
    "import bookquery\n",
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "ua = UserAgent()#Прокси\n",
    "curr_directory = os.getcwd()\n",
    "new_path = join(curr_directory,\"Information\")\n",
    "try:# Создаем базовую директорию для файлов\n",
    "    os.mkdir(new_path)\n",
    "except OSError:\n",
    "    print (\"Директория %s уже создана\" % new_path)\n",
    "    \n",
    "def execute_query(qquery, entity_id):# Работа с запросами\n",
    "    \n",
    "    \"\"\"Работа с запросами\"\"\"\n",
    "    \n",
    "    sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\",agent = ua.random)\n",
    "    sparql.setQuery(qquery.format(entity_id = entity_id))\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    results = sparql.query().convert()\n",
    "    return results['results']['bindings']\n",
    "\n",
    "def counter_for_files(path):\n",
    "    \n",
    "    \"\"\"Подсчет файлов в директории\"\"\"\n",
    "        \n",
    "    onlyfiles = [join(path, f) for f in listdir(path) if isfile(join(path, f))]\n",
    "    return onlyfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Директория D:\\Univer\\Сбербанк\\Work\\Geography\\Geography\\Information\\2\\data уже создана\n",
      "Директория D:\\Univer\\Сбербанк\\Work\\Geography\\Geography\\Information\\2\\data уже создана\n",
      "Сущность Q55488 уже обработана\n",
      "Сущность Q15104915 уже обработана\n",
      "Сущность Q3240715 уже обработана\n",
      "Сущность Q532 уже обработана\n",
      "Сущность Q835714 уже обработана\n",
      "Сущность Q515 уже обработана\n",
      "Сущность Q62447 уже обработана\n",
      "Сущность Q831740 уже обработана\n",
      "Сущность Q3957 уже обработанаСущность Q820477 уже обработана\n",
      "\n",
      "Сущность Q39816 уже обработанаСущность Q23442 уже обработана\n",
      "\n",
      "Сущность Q855697 уже обработана\n",
      "Сущность Q523166 уже обработана\n",
      "Сущность Q10354598 уже обработана\n",
      "Сущность Q8514 уже обработана\n",
      "Сущность Q399984 уже обработана\n",
      "Сущность Q2418896 уже обработана\n",
      "Сущность Q1349255 уже обработана\n",
      "Сущность Q358 уже обработана\n",
      "Сущность Q2989457 уже обработана\n",
      "Сущность Q1248784 уже обработана\n",
      "Сущность Q35657 уже обработанаСущность Q184358 уже обработана\n",
      "\n",
      "Сущность Q8502 уже обработана\n",
      "Сущность Q4421 уже обработана\n",
      "Сущность Q309166 уже обработана\n",
      "Сущность Q34763 уже обработана\n",
      "Сущность Q1233637 уже обработана\n",
      "Сущность Q82794 уже обработана\n",
      "Сущность Q184122 уже обработана\n",
      "Сущность Q319714 уже обработана\n",
      "Сущность Q3253281 уже обработана\n",
      "Сущность Q34038 уже обработана\n",
      "Сущность Q748331 уже обработана\n",
      "Сущность Q33506 уже обработана\n",
      "Сущность Q160091 уже обработана\n",
      "Сущность Q185113 уже обработана\n",
      "Сущность Q39594 уже обработана\n",
      "Сущность Q9430 уже обработана\n",
      "Сущность Q211748 уже обработанаСущность Q570116 уже обработана\n",
      "\n",
      "Сущность Q165 уже обработана\n",
      "Сущность Q45776 уже обработана\n",
      "Сущность Q35666 уже обработана\n",
      "Сущность Q1210950 уже обработана\n",
      "Сущность Q6256 уже обработана\n",
      "Сущность Q3336843 уже обработана\n",
      "Сущность Q1061151 уже обработанаСущность Q1404150 уже обработана\n",
      "\n",
      "Сущность Q35509 уже обработана\n",
      "Сущность Q1437459 уже обработана\n",
      "Сущность Q183273 уже обработана\n",
      "Сущность Q4022 уже обработана\n",
      "Сущность Q12284 уже обработана\n",
      "Сущность Q5107 уже обработана\n",
      "Сущность Q150784 уже обработана\n",
      "Сущность Q33837 уже обработана\n",
      "Сущность Q1402592 уже обработана\n",
      "Сущность Q8072 уже обработана\n",
      "Сущность Q46831 уже обработана\n",
      "Сущность Q5926864 уже обработана\n",
      "Сущность Q1081138 уже обработана\n",
      "Сущность Q23397 уже обработана\n",
      "Сущность Q7275 уже обработана\n",
      "Сущность Q22698 уже обработана\n",
      "Сущность Q183342 уже обработана\n",
      "Сущность Q1093829 уже обработанаСущность Q119253 уже обработана\n",
      "\n",
      "Сущность Q7930989 уже обработана\n",
      "Сущность Q12280 уже обработана\n",
      "Сущность Q483453 уже обработана\n",
      "Сущность Q174782 уже обработана\n",
      "Сущность Q41162 уже обработана\n",
      "Сущность Q166620 уже обработана\n",
      "Сущность Q4989906 уже обработана\n",
      "Сущность Q83471 уже обработана\n",
      "Сущность Q37901 уже обработана\n"
     ]
    }
   ],
   "source": [
    "ua = UserAgent()\n",
    "res = []\n",
    "bad_requests = []# массив для ненайденных\n",
    "\n",
    "def entities_sparqul(results):#\n",
    "    \n",
    "    \"\"\"Обработка результата\"\"\"\n",
    "    \n",
    "    res = []\n",
    "    for result in results:# Обработка вернувшегося запрсов\n",
    "        entities = {}\n",
    "        entities['entity'] = result['item']['value'].split('/')[-1]\n",
    "        entities['label'] = result['item']['value'].split('/')[-1]\n",
    "        showLabel = results[0]['showLabel']['value']\n",
    "        res.append(entities)# Создаем массив словарей\n",
    "    return pd.DataFrame(data = res),showLabel#Возвращаем Датафрейм и название сущности\n",
    "\n",
    "def file_creation(entity_id , path, f = 0):\n",
    "    \n",
    "    \"\"\"Создание файла со всеми сущностями\"\"\"\n",
    "    \n",
    "    global bad_requests\n",
    "    if f > 3:\n",
    "            bad_requests.append(entity_id)\n",
    "            return None\n",
    "    try:\n",
    "        new_path = join(curr_directory,join(\"Information\",path + \"\\\\data\"))\n",
    "        if entity_id not in list(map(lambda x:x.split('\\\\')[-1].\n",
    "                                     split('_')[-1].split('.')[0], counter_for_files(new_path))):#Проверка на наличие файла\n",
    "            raw_results= execute_query(bookquery.entity_finding, entity_id)# Запрос\n",
    "            result,Label = entities_sparqul(raw_results) # Обработка\n",
    "            result.to_csv(join(new_path ,f'entities_{Label}_{entity_id}.csv'),\n",
    "                          encoding='utf-8-sig',index = False,sep = ';')# Формирование\n",
    "            print(f'Cущность {Label} закончила обрабатываться')\n",
    "        else :\n",
    "            print(f'Сущность {entity_id} уже обработана')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        try:#Обработка внутренних ошибок\n",
    "            if e.code == -1003:\n",
    "                time.sleep(3)\n",
    "                print('Too many requests')\n",
    "                return file_creation(entity_id, f + 1)\n",
    "        except Exception as e1:\n",
    "            bad_requests.append(entity_id)\n",
    "            return None     \n",
    "        \n",
    "def global_file_creation(path):\n",
    "    \"\"\"Функция для получения ID из файла Entites.xlsx и создания файлов со всеми сущностями\"\"\"\n",
    "    curr_directory = os.getcwd()\n",
    "    csv_dir = join(curr_directory,'csv/Entities.xlsx')\n",
    "    new_path = join(curr_directory,join(\"Information\",path + f\"\\data\"))\n",
    "    new_path_1 = join(curr_directory,join(\"Information\",path))\n",
    "    try:# Создаем базовую директорию для файлов\n",
    "        os.mkdir(new_path_1)\n",
    "    except OSError:\n",
    "        print (\"Директория %s уже создана\" % new_path)\n",
    "    try:# Создаем базовую директорию для файлов\n",
    "        os.mkdir(new_path)\n",
    "    except OSError:\n",
    "        print (\"Директория %s уже создана\" % new_path)\n",
    "    exl = pd.read_excel(csv_dir)\n",
    "    exl = exl[exl['subclass_weight'] == int(path)]\n",
    "    for i in exl['subclass']:\n",
    "        res.append(dask.delayed(file_creation)(i,path))\n",
    "    final_dataframe = dask.compute(*res)\n",
    "\n",
    "global_file_creation('2')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entities_sparqul_new(results):\n",
    "    \n",
    "    \"\"\"Обработка результата\"\"\"\n",
    "    \n",
    "    res = []\n",
    "    for result in results:# Обработка вернувшегося запрсов\n",
    "        entities = {}\n",
    "        entities['entity'] = result['item']['value'].split('/')[-1]\n",
    "        entities['label'] = result['item']['value'].split('/')[-1]\n",
    "        res.append(entities)# Создаем массив словарей\n",
    "    return pd.DataFrame(data = res)#Возвращаем Датафрейм и название сущности\n",
    "\n",
    "\n",
    "def file_creation_new(entity_id , Label , path, f = 0)\n",
    "\n",
    "    \"\"\"Обработка крупных файлов(свыше 100000 представителей)\"\"\"\n",
    "    \n",
    "    global bad_requests\n",
    "    if f > 3:\n",
    "            bad_requests.append(entity_id)\n",
    "            return None\n",
    "    try:\n",
    "        new_path = join(curr_directory,join(\"Information\",path + \"\\\\data\"))\n",
    "        if entity_id not in list(map(lambda x:x.split('\\\\')[-1].\n",
    "                                     split('_')[-1].split('.')[0], counter_for_files(new_path))):#Проверка на наличие файла\n",
    "            raw_results= execute_query(bookquery.huge_entity_finding, entity_id)# Запрос\n",
    "            result = entities_sparqul_new(raw_results)# Обработка\n",
    "            result.to_csv(join(new_path ,f'entities_{Label}_{entity_id}.csv'),\n",
    "                          encoding='utf-8-sig',index = False,sep = ';')# Формирование\n",
    "        else :\n",
    "            print(f'Сущность {entity_id} уже обработана')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        try:#Обработка внутренних ошибок\n",
    "            if e.code == -1003:\n",
    "                time.sleep(3)\n",
    "                print('Too many requests')\n",
    "                return file_creation(entity_id, f + 1)\n",
    "        except Exception as e1:\n",
    "            bad_requests.append(entity_id)\n",
    "            return None     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_creation_new('Q8502' , 'mountain' ,'2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Директория D:\\Univer\\Сбербанк\\Work\\Geography\\Geography\\Information\\2\\data\\data_with_popularity уже создана\n",
      "Файл aerodrome уже обработан\n",
      "Файл airport уже обработан\n",
      "Файл archipelago уже обработан\n",
      "Файл autonomous oblast of Russia уже обработан\n",
      "Файл autonomous okrug of Russia уже обработан\n",
      "Файл bay уже обработан\n",
      "Файл bridge уже обработан\n",
      "Файл canal уже обработан\n",
      "Файл canyon уже обработан\n",
      "Файл cape уже обработан\n",
      "Файл cave уже обработан\n",
      "Файл channel уже обработан\n",
      "Файл city of the United States уже обработан\n",
      "Файл city town уже обработан\n",
      "Файл city уже обработан\n",
      "Файл constituent part of the United Kingdom уже обработан\n",
      "Файл continental area and surrounding islands уже обработан\n",
      "Файл continent уже обработан\n",
      "Файл country уже обработан\n",
      "Файл crater уже обработан\n",
      "Файл desert уже обработан\n",
      "Файл drainage basin уже обработан\n",
      "Файл federal city of Russia уже обработан\n",
      "Файл fjard уже обработан\n",
      "Файл fjord уже обработан\n",
      "Файл forest уже обработан\n",
      "Файл fountain уже обработан\n",
      "Файл geographic region уже обработан\n",
      "Файл geographical pole уже обработан\n",
      "Файл geyser уже обработан\n",
      "Файл glacier уже обработан\n",
      "Файл gracht уже обработан\n",
      "Файл group of lakes уже обработан\n",
      "Файл hemisphere of the Earth уже обработан\n",
      "Файл heritage site уже обработан\n",
      "Файл historic site уже обработан\n",
      "Файл island group уже обработан\n",
      "Файл island уже обработан\n",
      "Файл krai of Russia уже обработан\n",
      "Файл lake уже обработан\n",
      "Файл massif уже обработан\n",
      "Файл mineral deposit уже обработан\n",
      "Файл mine уже обработан\n",
      "Файл monument уже обработан\n",
      "Файл mountain range уже обработан\n",
      "Файл mountain уже обработан\n",
      "Файл museum уже обработан\n",
      "Файл natural gas field уже обработан\n",
      "Файл non-geologically related mountain range уже обработан\n",
      "Файл oblast of Russia уже обработан\n",
      "Файл oceanic trench уже обработан\n",
      "Файл ocean уже обработан\n",
      "Файл oil field уже обработан\n",
      "Файл park уже обработан\n",
      "Файл peninsula уже обработан\n",
      "Файл plain уже обработан\n",
      "Файл pond уже обработан\n",
      "Файл railway station уже обработан\n",
      "Файл reef уже обработан\n",
      "Файл republic of Russia уже обработан\n",
      "Файл river mouth уже обработан\n",
      "Файл river уже обработан\n",
      "Файл rock уже обработан\n",
      "Файл rural settlement уже обработан\n",
      "Файл sea уже обработан\n",
      "Файл settlement уже обработан\n",
      "Файл square уже обработан\n",
      "Файл stanitsa уже обработан\n",
      "Файл state of the United States уже обработан\n",
      "Файл state уже обработан\n",
      "Файл strait уже обработан\n",
      "Файл subcontinent уже обработан\n",
      "Файл tourist attraction уже обработан\n",
      "Файл town уже обработан\n",
      "Файл urban-type settlement уже обработан\n",
      "Файл valley уже обработан\n",
      "Файл volcano уже обработан\n",
      "Файл waterfall уже обработан\n",
      "Wall time: 679 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def counter_popularity(v , f = 0,entity_id = None,label  = None):\n",
    "    \n",
    "    \"\"\"Функция возвращает популярность для каждой сущности\"\"\"\n",
    "    \n",
    "    if f > 25:#Если не удается нормально проработать , то записываем -1\n",
    "        new_entity = {\n",
    "            'entity':entity_id,\n",
    "            'label':label,\n",
    "            'popularity' : -1\n",
    "        }\n",
    "        return new_entity\n",
    "    try:\n",
    "        new_entity = {}\n",
    "        entity_id, label = v['entity'], v['label']\n",
    "        from_res =int(execute_query(bookquery.counter_from, entity_id)[0]['countOut']['value'])#Исходящие ссылки\n",
    "        to_res =int(execute_query(bookquery.counter_to, entity_id)[0]['countIn']['value']) #Входящие ссылки\n",
    "        new_entity = {\n",
    "            'entity':entity_id,\n",
    "            'label':label,\n",
    "            'popularity' : from_res + to_res#общая популярность\n",
    "        }\n",
    "        return  new_entity\n",
    "    except Exception as e:#перехватываем ошибку и слипим на 3 секунды для более корректной работы endpoint\n",
    "        time.sleep(3)\n",
    "        return counter_popularity(v, f + 1,entity_id, label)\n",
    "        \n",
    "        #Пушим в массив необработанные сущности\n",
    "        \n",
    "    \n",
    "def determ_popul(filename,path):\n",
    "    \n",
    "    \"\"\"Подсчет популрностей дял всех сущностей\"\"\"\n",
    "    \n",
    "    curr_directory = os.getcwd()\n",
    "    new_path = join(curr_directory,join(\"Information\",path + \"\\\\data\\data_with_popularity\"))\n",
    "    labels = filename.split('\\\\')[-1].split('_')#название класса\n",
    "    label = labels[-2]\n",
    "    entity_id = labels[-1].split('.')[0]\n",
    "    if entity_id not in list(map (lambda x : x.split('_')[-3],counter_for_files(new_path))):# проверка на наличие     \n",
    "        start = time.time()\n",
    "        file_name = filename.split('\\\\')[-1].split('.')[0]\n",
    "        print(f'Файл {label} начал обрабатываться {datetime.datetime.now().time()}')\n",
    "        res = []\n",
    "        result = []\n",
    "        ids = pd.read_csv(filename, encoding='utf8',sep = ';')#читаем файл\n",
    "        for k, v in ids.iterrows():#Запросы по каждой сущности\n",
    "            res.append(dask.delayed(counter_popularity)(v))\n",
    "#             result.append(counter_popularity(v))\n",
    "        if not res:\n",
    "            print(f\"Файл {label} является пустым\")\n",
    "            return None\n",
    "        result = pd.DataFrame(data = dask.compute(*res))\n",
    "#         result = pd.DataFrame(data = result)\n",
    "        result.sort_values(by = 'popularity',ascending = False).to_csv(\n",
    "            join(new_path,file_name + '_with_popularity.csv'),\n",
    "            encoding='utf-8-sig',index = False,sep = ';')# Формирование\n",
    "        end = time.time()\n",
    "        print(f'Формирование файла {file_name} окончено , заняло {(end-start)/60} минут\\n')\n",
    "    else:\n",
    "        print(f'Файл {label} уже обработан')\n",
    "\n",
    "def global_finding_popularity(path):\n",
    "    \n",
    "    \"\"\"Проход по всем файлам в директории\"\"\"\n",
    "    \n",
    "    res = []\n",
    "    curr_directory = os.getcwd()\n",
    "    new_path = join(curr_directory,join(\"Information\",path + \"\\\\data\\data_with_popularity\"))\n",
    "    path_for_finding = join(curr_directory,join(\"Information\",path + \"\\\\data\"))\n",
    "    try:\n",
    "        os.mkdir(new_path)\n",
    "    except OSError:\n",
    "        print (\"Директория %s уже создана\" % new_path)\n",
    "        \n",
    "    for filename in counter_for_files(path_for_finding):\n",
    "        determ_popul(filename,path)\n",
    "    #result = dask.compute(*res)\n",
    "\n",
    "global_finding_popularity(\"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Директория D:\\Univer\\Сбербанк\\Work\\Geography\\Geography\\Information\\2\\analyzing уже создана\n",
      "Директория D:\\Univer\\Сбербанк\\Work\\Geography\\Geography\\Information\\2\\analyzing\\aggregated уже создана\n",
      "Директория D:\\Univer\\Сбербанк\\Work\\Geography\\Geography\\Information\\2\\analyzing\\aggregated_with_popularity уже создана\n",
      "Файл aerodrome уже обработан\n",
      "Файл airport уже обработан\n",
      "Файл archipelago уже обработан\n",
      "Файл autonomous oblast of Russia уже обработан\n",
      "Файл autonomous okrug of Russia уже обработан\n",
      "Файл bay уже обработан\n",
      "Файл bridge уже обработан\n",
      "Файл canal уже обработан\n",
      "Файл canyon уже обработан\n",
      "Файл cape уже обработан\n",
      "Файл cave уже обработан\n",
      "Файл channel уже обработан\n",
      "Файл city of the United States уже обработан\n",
      "Файл city town уже обработан\n",
      "Файл city уже обработан\n",
      "Файл constituent part of the United Kingdom уже обработан\n",
      "Файл continental area and surrounding islands уже обработан\n",
      "Файл continent уже обработан\n",
      "Файл country уже обработан\n",
      "Файл crater уже обработан\n",
      "Файл desert уже обработан\n",
      "Файл drainage basin уже обработан\n",
      "Файл federal city of Russia уже обработан\n",
      "Файл fjard уже обработан\n",
      "Файл fjord уже обработан\n",
      "Файл forest уже обработан\n",
      "Файл fountain уже обработан\n",
      "Файл geographic region уже обработан\n",
      "Файл geographical pole уже обработан\n",
      "Файл geyser уже обработан\n",
      "Файл glacier уже обработан\n",
      "Файл gracht уже обработан\n",
      "Файл group of lakes уже обработан\n",
      "Файл hemisphere of the Earth уже обработан\n",
      "Файл heritage site уже обработан\n",
      "Файл historic site уже обработан\n",
      "Файл island group уже обработан\n",
      "Файл island уже обработан\n",
      "Файл krai of Russia уже обработан\n",
      "Файл lake уже обработан\n",
      "Файл massif уже обработан\n",
      "Файл mineral deposit уже обработан\n",
      "Файл mine уже обработан\n",
      "Файл monument уже обработан\n",
      "Файл mountain range уже обработан\n",
      "Файл mountain уже обработан\n",
      "Файл museum уже обработан\n",
      "Файл natural gas field уже обработан\n",
      "Файл non-geologically related mountain range уже обработан\n",
      "Файл oblast of Russia уже обработан\n",
      "Файл oceanic trench уже обработан\n",
      "Файл ocean уже обработан\n",
      "Файл oil field уже обработан\n",
      "Файл park уже обработан\n",
      "Файл peninsula уже обработан\n",
      "Файл plain уже обработан\n",
      "Файл pond уже обработан\n",
      "Файл railway station уже обработан\n",
      "Файл reef уже обработан\n",
      "Файл republic of Russia уже обработан\n",
      "Файл river mouth уже обработан\n",
      "Файл river уже обработан\n",
      "Файл rock уже обработан\n",
      "Файл rural settlement уже обработан\n",
      "Файл sea уже обработан\n",
      "Файл settlement уже обработан\n",
      "Файл square уже обработан\n",
      "Файл stanitsa уже обработан\n",
      "Файл state of the United States уже обработан\n",
      "Файл state уже обработан\n",
      "Файл strait уже обработан\n",
      "Файл subcontinent уже обработан\n",
      "Файл tourist attraction уже обработан\n",
      "Файл town уже обработан\n",
      "Файл urban-type settlement уже обработан\n",
      "Файл valley уже обработан\n",
      "Файл volcano уже обработан\n",
      "Файл waterfall уже обработан\n"
     ]
    }
   ],
   "source": [
    "def execute_query(qquery, book_id):\n",
    "    \n",
    "    \"\"\"Обработка результата\"\"\"\n",
    "    \n",
    "    sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\",agent = ua.random)\n",
    "    sparql.setQuery(qquery.format(book_id = book_id))\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    results = sparql.query().convert()\n",
    "    return results['results']['bindings']\n",
    "\n",
    "def entity_from_sparql(results):\n",
    "    \n",
    "    \"\"\"Обработка исходящих предикатов\"\"\"\n",
    "    \n",
    "    entity = {}\n",
    "    for result in results:#Построчно читаем\n",
    "        wdLabel, ps_Label, ps_, wd, direction = result['wdLabel']['value'], result['ps_Label']['value'], \\\n",
    "                        result['ps_']['value'], result['wd']['value'], result['direction']['value']\n",
    "        ps = {\n",
    "            'propValueID': ps_,\n",
    "            'propValue': ps_Label\n",
    "        }\n",
    "        if wdLabel in entity:# Формируем словари для повторяющихся предикатов\n",
    "                entity[wdLabel]['content'].append(ps)\n",
    "        else:\n",
    "            entity[wdLabel] = {\n",
    "                'propID': wd,\n",
    "                'direction': direction,\n",
    "                'content': [ps]\n",
    "            }\n",
    "    return entity\n",
    "\n",
    "def entity_to_sparql(results):\n",
    "    \n",
    "    \"\"\"Обрабокта входящих предикатов\"\"\"\n",
    "    \n",
    "    entity = {}\n",
    "    for result in results:\n",
    "        obj, objLabel, wd, prop_label, bookLabel, direction = result['obj']['value'], result['objLabel']['value'], \\\n",
    "                        result['wd']['value'], result['prop_label']['value'], \\\n",
    "                        result['bookLabel']['value'], result['direction']['value']\n",
    "        ps = {\n",
    "            'propValueID': obj,\n",
    "            'propValue': objLabel\n",
    "        }\n",
    "        \n",
    "        if prop_label in entity:\n",
    "                entity[prop_label]['content'].append(ps)\n",
    "        else:\n",
    "            entity[prop_label] = {\n",
    "                'propID': wd,\n",
    "                'direction': direction,\n",
    "                'content': [ps]\n",
    "            }\n",
    "    return entity\n",
    "    \n",
    "\n",
    "def get_properties_of_book(book_id, book_name=None):\n",
    "    \n",
    "    \"\"\"Формирование датафрейма\"\"\"\n",
    "    \n",
    "    results_from = execute_query(bookquery.query_from, book_id)\n",
    "    results_to = execute_query(bookquery.query_to, book_id)\n",
    "    \n",
    "    entity_from = entity_from_sparql(results_from)\n",
    "    entity_to = entity_to_sparql(results_to)\n",
    "    \n",
    "    entity = {**entity_from, **entity_to}# Объединяем словарь \n",
    "#     _book_name = book_name if book_name is not None else \\\n",
    "#                 (entity['title'][0]['ps_Label'] if 'title' in entity else book_id)\n",
    "    _book_name = book_name\n",
    "    \n",
    "    counter, res = {}, []\n",
    "    for k, v in entity.items(): \n",
    "        counter[k] = len(v['content'])\n",
    "    for k, v in counter.items():# Формирвоание датафрейма\n",
    "        prop = {\n",
    "            'entity_id': book_id,\n",
    "            'entity_name': _book_name,\n",
    "            'prop_name': k,\n",
    "            'prop_count':1,\n",
    "            'prop_count_value': v,\n",
    "            'prop_dir': entity[k]['direction']\n",
    "        }\n",
    "        res.append(prop)\n",
    "    return pd.DataFrame(res)\n",
    "\n",
    "def processing(v, f=0):\n",
    "    \n",
    "    \"\"\"Обработка запроса\"\"\"\n",
    "    \n",
    "    if f > 20:# Если не удается , то ничего не возвращаем\n",
    "            return None\n",
    "    try:\n",
    "        book_id, bookLabel = v['entity'], v['label']\n",
    "        dataframe = get_properties_of_book(book_id, book_name=bookLabel)\n",
    "        \n",
    "        return  dataframe\n",
    "    except Exception as e:\n",
    "        time.sleep(3)\n",
    "        return processing(v, f + 1)\n",
    "\n",
    "def finding_popular(most_popular):\n",
    "        \n",
    "    \"\"\"Обработка 5% наиболее популярных сущностей\"\"\"\n",
    "        \n",
    "    if len(most_popular) * 0.05 < 10:\n",
    "        return list(most_popular.iloc[:10].entity)\n",
    "    elif len(most_popular) * 0.05 >= 100:\n",
    "        return list(most_popular.iloc[:100].entity)\n",
    "    else:\n",
    "        return list(most_popular.iloc[:int(len(most_popular) * 0.05)].entity)\n",
    "        \n",
    "def entity_analyzer(filename,path):\n",
    "    \n",
    "    \"\"\"Создание csv-файлов\"\"\"\n",
    "        \n",
    "    curr_path = join(curr_directory,join(\"Information\",path + \"\\\\analyzing\")) # Обработчик файлов\n",
    "    Label = filename.split('\\\\')[-1].split('.')[0].split('_')[-1]# Название файла\n",
    "    file_name = filename.split('\\\\')[-1].split('.')[0]\n",
    "    label  = filename.split('\\\\')[-1].split('.')[0].split('_')[-2]\n",
    "    if Label not in list (map(lambda x:x.split('\\\\')[-1].split('.')[0].split('_')[-1],counter_for_files(curr_path))):\n",
    "        print(f'Файл {label} начал обрабатываться {datetime.datetime.now().time()}')\n",
    "        start = time.time()\n",
    "        ids = pd.read_csv(filename, encoding='utf8',sep = ';')\n",
    "        stats_dfs = []\n",
    "        entities = []\n",
    "        res = []\n",
    "        result = []\n",
    "        for k, v in ids.iterrows():#Запросы по каждой сущности\n",
    "#             result.append(processing(v))\n",
    "            res.append(dask.delayed(processing)(v))\n",
    "        if res == []:\n",
    "            print(f'Файл {label} пуст')\n",
    "            return None\n",
    "        result = dask.compute(*res)\n",
    "        errors_counter = 0\n",
    "        for i in range(len(result)):#Формируем датафреймы\n",
    "            try:\n",
    "                stats_dfs.append(result[i])\n",
    "            except Exception as e:\n",
    "                errors_counter+=1\n",
    "                continue\n",
    "        #Создание файла со всеми предикатами для каждой сущности        \n",
    "        new_path = join(curr_directory,join(\"Information\",path + \"\\\\data\\data_with_popularity\"))\n",
    "        stats_df = pd.concat(stats_dfs)#Создание файлов\n",
    "        stats_df.sort_values(by = 'prop_count',ascending = False).to_csv(\n",
    "            curr_path + f'/analyzed_{file_name}.csv', encoding='utf-8-sig', index=False,sep = ';')\n",
    "\n",
    "        # Создание агрегированного по сущностям файла для предикатов\n",
    "        stats_aggregated = stats_df.groupby('prop_name').sum().sort_values(\n",
    "            by = ['prop_count','prop_count_value'],ascending = False)\n",
    "        stats_aggregated['prop_counter'] = stats_aggregated['prop_count']/(len(ids)-errors_counter) * 100\n",
    "        stats_aggregated.to_csv(curr_path + f'/aggregated/{file_name}_aggregated.csv', encoding='utf-8-sig',sep = ';')\n",
    "\n",
    "        most_popular = pd.read_csv(join(new_path,f'{file_name}_with_popularity.csv'),encoding='utf8',sep=';')\n",
    "        most_popular = finding_popular(most_popular)\n",
    "        def filter_func(x):\n",
    "            return x['entity_id'].max() in most_popular\n",
    "        #Топ 5% популярных сущностей\n",
    "        popular_stats = stats_df.groupby('entity_id').filter(filter_func).groupby('prop_name').sum().sort_values(\n",
    "            by = ['prop_count','prop_count_value'],ascending = False)\n",
    "        popular_stats['prop_counter'] = popular_stats['prop_count']/(len(most_popular)-errors_counter) * 100\n",
    "        popular_stats.to_csv(curr_path + f'/aggregated_with_popularity/{file_name}_aggregated_with_popularity.csv',\n",
    "                             encoding='utf-8-sig',sep = ';')\n",
    "\n",
    "        end = time.time()\n",
    "        print(f'Файл {file_name} отработал:количество необработанных файлов {errors_counter} из {len(ids)}(успех:{(len(ids)-errors_counter)*100/(len(ids))} %), заняло {(end-start)/60} минут')\n",
    "    else:\n",
    "        print(f'Файл {label} уже обработан')\n",
    "\n",
    "\n",
    "        \n",
    "def global_analyzer(path):\n",
    "    \n",
    "    \"\"\"Функция обхода по всем файлам директории\"\"\"\n",
    "    \n",
    "    res = []\n",
    "    curr_directory = os.getcwd()\n",
    "    \n",
    "    new_path_main = join(curr_directory,join(\"Information\",path + r\"\\analyzing\"))\n",
    "    path_for_finding = join(curr_directory,join(\"Information\",path + r\"\\data\"))\n",
    "    try:\n",
    "        os.mkdir(new_path_main)\n",
    "    except OSError:\n",
    "        print (\"Директория %s уже создана\" % new_path_main)\n",
    "\n",
    "    new_path = join(new_path_main,'aggregated')\n",
    "    try:\n",
    "        os.mkdir(new_path)\n",
    "    except OSError:\n",
    "        print (\"Директория %s уже создана\" % new_path)\n",
    "\n",
    "    new_path = join(new_path_main,'aggregated_with_popularity')\n",
    "    try:\n",
    "        os.mkdir(new_path)\n",
    "    except OSError:\n",
    "        print (\"Директория %s уже создана\" % new_path)\n",
    "    for filename in counter_for_files(path_for_finding):\n",
    "        entity_analyzer(filename,path)\n",
    "        #res.append(dask.delayed( entity_analyzer)(filename,path))\n",
    "    #result = dask.compute(*res)\n",
    "\n",
    "\n",
    "global_analyzer(\"2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
