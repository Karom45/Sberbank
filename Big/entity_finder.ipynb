{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install fake-useragent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install SPARQLWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Директория C:\\Users\\Алексей Кушнер\\Sberbank\\Geography\\Big\\Information уже создана\n"
     ]
    }
   ],
   "source": [
    "from fake_useragent import UserAgent\n",
    "from os.path import join,isfile\n",
    "from os import listdir\n",
    "import dask\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import bookquery\n",
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "import math\n",
    "\n",
    "ua = UserAgent()#Прокси\n",
    "curr_directory = os.getcwd()\n",
    "new_path = join(curr_directory,\"Information\")\n",
    "try:# Создаем базовую директорию для файлов\n",
    "    os.mkdir(new_path)\n",
    "except OSError:\n",
    "    print (\"Директория %s уже создана\" % new_path)\n",
    "    \n",
    "def execute_query(qquery, entity_id):\n",
    "    \n",
    "    \"\"\"Работа с запросами\"\"\"\n",
    "    \n",
    "    sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\",agent = ua.random)\n",
    "    sparql.setQuery(qquery.format(entity_id = entity_id))\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    results = sparql.query().convert()\n",
    "    return results['results']['bindings']\n",
    "\n",
    "def counter_for_files(path):\n",
    "    \n",
    "    \"\"\"Подсчет файлов в директории\"\"\"\n",
    "    \n",
    "    \n",
    "    onlyfiles = [join(path, f) for f in listdir(path) if isfile(join(path, f))]\n",
    "    return onlyfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Директория C:\\Users\\Алексей Кушнер\\Sberbank\\Geography\\Big\\Information\\2\\data\\data_with_popularity уже создана\n",
      "Файл country начал обрабатываться 12:00:15.951563\n",
      "Файл entities_country_Q6256_with_popularity_0 начал обрабатываться 12:00:15.954562\n",
      "EndPointInternalError: endpoint returned code 500 and response. \n",
      "\n",
      "Response:\n",
      "b'SPARQL-QUERY: queryStr=\\nSELECT  (COUNT(?obj) AS ?countIn) \\n\\t\\t\\tWHERE {\\n\\t\\t\\t  VALUES (?book) {(wd:Q30)} .\\n\\t\\t\\t  ?obj ?prop_id ?book.\\n\\t\\t\\t  ?wd wikibase:directClaim ?prop_id.\\n\\t\\t\\t  ?wd rdfs:label ?prop_label.\\n\\t\\t\\t  FILTER((LANG(?prop_label)) = \"ru\").\\n\\t\\t\\t}\\n\\njava.util.concurrent.TimeoutException\\n\\tat java.util.concurrent.FutureTask.get(FutureTask.java:205)\\n\\tat com.bigdata.rdf.sail.webapp.BigdataServlet.submitApiTask(BigdataServlet.java:292)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet.doSparqlQuery(QueryServlet.java:678)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet.doGet(QueryServlet.java:290)\\n\\tat com.bigdata.rdf.sail.webapp.RESTServlet.doGet(RESTServlet.java:240)\\n\\tat com.bigdata.rdf.sail.webapp.MultiTenancyServlet.doGet(MultiTenancyServlet.java:273)\\n\\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\\n\\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\\n\\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:865)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1655)\\n\\tat org.wikidata.query.rdf.blazegraph.throttling.ThrottlingFilter.doFilter(ThrottlingFilter.java:320)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.throttling.SystemOverloadFilter.doFilter(SystemOverloadFilter.java:82)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat ch.qos.logback.classic.helpers.MDCInsertingServletFilter.doFilter(MDCInsertingServletFilter.java:49)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.QueryEventSenderFilter.doFilter(QueryEventSenderFilter.java:93)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.ClientIPFilter.doFilter(ClientIPFilter.java:43)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.RealAgentFilter.doFilter(RealAgentFilter.java:33)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1634)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:533)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:146)\\n\\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:257)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1595)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1340)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:473)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1564)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1242)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)\\n\\tat org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:220)\\n\\tat org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:126)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\\n\\tat org.eclipse.jetty.server.Server.handle(Server.java:503)\\n\\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:364)\\n\\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:260)\\n\\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)\\n\\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\\n\\tat org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:118)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)\\n\\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:765)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:683)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n'\n",
      "EndPointInternalError: endpoint returned code 500 and response. \n",
      "\n",
      "Response:\n",
      "b'SPARQL-QUERY: queryStr=\\nSELECT  (COUNT(?obj) AS ?countIn) \\n\\t\\t\\tWHERE {\\n\\t\\t\\t  VALUES (?book) {(wd:Q17)} .\\n\\t\\t\\t  ?obj ?prop_id ?book.\\n\\t\\t\\t  ?wd wikibase:directClaim ?prop_id.\\n\\t\\t\\t  ?wd rdfs:label ?prop_label.\\n\\t\\t\\t  FILTER((LANG(?prop_label)) = \"ru\").\\n\\t\\t\\t}\\n\\njava.util.concurrent.TimeoutException\\n\\tat java.util.concurrent.FutureTask.get(FutureTask.java:205)\\n\\tat com.bigdata.rdf.sail.webapp.BigdataServlet.submitApiTask(BigdataServlet.java:292)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet.doSparqlQuery(QueryServlet.java:678)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet.doGet(QueryServlet.java:290)\\n\\tat com.bigdata.rdf.sail.webapp.RESTServlet.doGet(RESTServlet.java:240)\\n\\tat com.bigdata.rdf.sail.webapp.MultiTenancyServlet.doGet(MultiTenancyServlet.java:273)\\n\\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\\n\\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\\n\\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:865)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1655)\\n\\tat org.wikidata.query.rdf.blazegraph.throttling.ThrottlingFilter.doFilter(ThrottlingFilter.java:320)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.throttling.SystemOverloadFilter.doFilter(SystemOverloadFilter.java:82)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat ch.qos.logback.classic.helpers.MDCInsertingServletFilter.doFilter(MDCInsertingServletFilter.java:49)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.QueryEventSenderFilter.doFilter(QueryEventSenderFilter.java:93)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.ClientIPFilter.doFilter(ClientIPFilter.java:43)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.RealAgentFilter.doFilter(RealAgentFilter.java:33)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1634)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:533)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:146)\\n\\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:257)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1595)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1340)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:473)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1564)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1242)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)\\n\\tat org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:220)\\n\\tat org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:126)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\\n\\tat org.eclipse.jetty.server.Server.handle(Server.java:503)\\n\\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:364)\\n\\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:260)\\n\\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)\\n\\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\\n\\tat org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:118)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)\\n\\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:765)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:683)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Формирование файла entities_country_Q6256_with_popularity_0 окончено , заняло 4.661867006619771 минут\n",
      "\n",
      "Файл entities_country_Q6256_with_popularity_1 начал обрабатываться 12:04:55.666582\n",
      "Формирование файла entities_country_Q6256_with_popularity_1 окончено , заняло 6.658766269683838 минут\n",
      "\n",
      "Файл entities_country_Q6256_with_popularity_2 начал обрабатываться 12:11:35.192558\n",
      "Формирование файла entities_country_Q6256_with_popularity_2 окончено , заняло 17.094158029556276 минут\n",
      "\n",
      "Файл entities_country_Q6256_with_popularity_3 начал обрабатываться 12:28:40.842040\n",
      "Формирование файла entities_country_Q6256_with_popularity_3 окончено , заняло 11.723495014508565 минут\n",
      "\n",
      "Файл entities_country_Q6256_with_popularity_4 начал обрабатываться 12:40:24.251741\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-e98331369003>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;31m#result = dask.compute(*res)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m \u001b[0mglobal_finding_popularity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"2\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-e98331369003>\u001b[0m in \u001b[0;36mglobal_finding_popularity\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Директория %s уже создана\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mnew_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcounter_for_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_for_finding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m         \u001b[0mdeterm_popul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m     \u001b[1;31m#result = dask.compute(*res)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-e98331369003>\u001b[0m in \u001b[0;36mdeterm_popul\u001b[1;34m(filename, path)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mBLOCK_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mBLOCK_SIZE\u001b[0m\u001b[1;33m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m                 \u001b[0mcreating_parts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mBLOCK_SIZE\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mBLOCK_SIZE\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m                 \u001b[0mcreating_parts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mBLOCK_SIZE\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-e98331369003>\u001b[0m in \u001b[0;36mcreating_parts\u001b[1;34m(i, ids, file_name, path)\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Файл {label} является пустым\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m     \u001b[1;31m#       result = pd.DataFrame(data = result)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         result.sort_values(by = 'popularity',ascending = False).to_csv(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\dask\\base.py\u001b[0m in \u001b[0;36mcompute\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    434\u001b[0m     \u001b[0mkeys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dask_keys__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m     \u001b[0mpostcomputes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dask_postcompute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 436\u001b[1;33m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mschedule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdsk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    437\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mrepack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpostcomputes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\dask\\threaded.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(dsk, result, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0mget_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_thread_get_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[0mpack_exception\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpack_exception\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m         \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m     )\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\dask\\local.py\u001b[0m in \u001b[0;36mget_async\u001b[1;34m(apply_async, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, **kwargs)\u001b[0m\n\u001b[0;32m    473\u001b[0m             \u001b[1;31m# Main loop, wait on tasks to finish, insert new ones\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"waiting\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"ready\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"running\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 475\u001b[1;33m                 \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfailed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mqueue_get\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqueue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    476\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mfailed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m                     \u001b[0mexc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres_info\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\dask\\local.py\u001b[0m in \u001b[0;36mqueue_get\u001b[1;34m(q)\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\queue.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    177\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m                         \u001b[1;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m             \u001b[0mitem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def counter_popularity(v , f = 0,entity_id = None,label  = None):\n",
    "    \n",
    "    \"\"\"Функция возвращает популярность для каждой сущности\"\"\"\n",
    "    \n",
    "    if f > 10:#Если не удается нормально проработать , то записываем -1\n",
    "        new_entity = {\n",
    "            'entity':entity_id,\n",
    "            'label':label,\n",
    "            'popularity' : -1\n",
    "        }\n",
    "        return new_entity\n",
    "    try:\n",
    "        new_entity = {}\n",
    "        entity_id, label = v['entity'], v['label']\n",
    "        from_res =int(execute_query(bookquery.counter_from, entity_id)[0]['countOut']['value'])#Исходящие ссылки\n",
    "        to_res =int(execute_query(bookquery.counter_to, entity_id)[0]['countIn']['value']) #Входящие ссылки\n",
    "        new_entity = {\n",
    "            'entity':entity_id,\n",
    "            'label':label,\n",
    "            'popularity' : from_res + to_res#общая популярность\n",
    "        }\n",
    "        return  new_entity\n",
    "    except Exception as e:#перехватываем ошибку и слипим на 3 секунды для более корректной работы endpoint\n",
    "        time.sleep(3)\n",
    "        return counter_popularity(v, f + 1,entity_id, label)\n",
    "        \n",
    "    \n",
    "        \n",
    "\n",
    "def creating_parts(i, ids, file_name,path):\n",
    "    \n",
    "    \"\"\"Создает блоки по 50000 представителей с проставленной популярностью\"\"\"\n",
    "    \n",
    "    new_path = join(curr_directory,join(\"Information\",path +\"\\\\data\\\\data_with_popularity\"))\n",
    "    if str(i) not in list(map (lambda x : x.split('_')[-1].split('.')[0],counter_for_files(new_path))):\n",
    "        print(f'Файл {file_name}_with_popularity_{i} начал обрабатываться {datetime.datetime.now().time()}')\n",
    "        start = time.time()\n",
    "        res = []\n",
    "        result = []   \n",
    "        for k, v in ids.iterrows():#Запросы по каждой сущности\n",
    "            res.append(dask.delayed(counter_popularity)(v))\n",
    "    #           result.append(counter_popularity(v))\n",
    "        if not res:\n",
    "            print(f\"Файл {label} является пустым\")\n",
    "            return None\n",
    "        result = pd.DataFrame(data = dask.compute(*res))\n",
    "    #       result = pd.DataFrame(data = result)\n",
    "        result.sort_values(by = 'popularity',ascending = False).to_csv(\n",
    "                join(new_path,file_name + f'_with_popularity_{i}.csv'),\n",
    "                encoding='utf-8-sig',index = False,sep = ';')# Формирование\n",
    "        end = time.time()\n",
    "        print(f'Формирование файла {file_name}_with_popularity_{i} окончено , заняло {(end-start)/60} минут\\n')\n",
    "    else:\n",
    "        print(f'{i} файл уже обработан')\n",
    "\n",
    "def determ_popul(filename,path):\n",
    "    \n",
    "    \"\"\"Функция для нахождения популярности в каждом блоке\"\"\"\n",
    "    \n",
    "    BLOCK_SIZE = 50000 #Размер блока\n",
    "    curr_directory = os.getcwd()\n",
    "    new_path = join(curr_directory,join(\"Information\",path + \"\\\\data\\data_with_popularity\"))\n",
    "    labels = filename.split('\\\\')[-1].split('_')#название класса\n",
    "    label = labels[-2]\n",
    "    entity_id = labels[-1].split('.')[0]\n",
    "    if filename not in counter_for_files(new_path):# проверка на наличие     \n",
    "        start = time.time()\n",
    "        file_name = filename.split('\\\\')[-1].split('.')[0]\n",
    "        print(f'Файл {label} начал обрабатываться {datetime.datetime.now().time()}')\n",
    "        ids = pd.read_csv(filename, encoding='utf8',sep = ';')#читаем файл\n",
    "        \n",
    "        for i in range(math.ceil(len(ids)/BLOCK_SIZE)): # Обход всех элементов блока\n",
    "            if (i+1) *BLOCK_SIZE< len(ids):\n",
    "                creating_parts(i , ids[BLOCK_SIZE * i: BLOCK_SIZE*(i+1)] ,file_name,path)\n",
    "            else:\n",
    "                creating_parts(i , ids[BLOCK_SIZE * i:],file_name,path)\n",
    "        end = time.time()\n",
    "        \n",
    "        print(f'Формирование файла {file_name}_with_popularity окончено , заняло {(end-start)/60} минут\\n!!!!!!!!!!!!!!!!!')\n",
    "    else:\n",
    "        print(f'Файл {label} уже обработан')\n",
    "\n",
    "def global_finding_popularity(path):\n",
    "    \n",
    "    \"\"\"Обработка файла в директории\"\"\"\n",
    "    \n",
    "    res = []\n",
    "    curr_directory = os.getcwd()\n",
    "    new_path = join(curr_directory,join(\"Information\",path + \"\\\\data\\data_with_popularity\"))\n",
    "    path_for_finding = join(curr_directory,join(\"Information\",path + \"\\\\data\"))\n",
    "    try:\n",
    "        os.mkdir(new_path)\n",
    "    except OSError:\n",
    "        print (\"Директория %s уже создана\" % new_path)\n",
    "    for filename in counter_for_files(path_for_finding):\n",
    "        determ_popul(filename,path)\n",
    "    #result = dask.compute(*res)\n",
    "\n",
    "global_finding_popularity(\"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Директория C:\\Users\\Алексей Кушнер\\Sberbank\\Geography\\Big\\Information\\2\\analyzing уже создана\n",
      "Директория C:\\Users\\Алексей Кушнер\\Sberbank\\Geography\\Big\\Information\\2\\analyzing\\aggregated уже создана\n",
      "Директория C:\\Users\\Алексей Кушнер\\Sberbank\\Geography\\Big\\Information\\2\\analyzing\\aggregated_with_popularity уже создана\n",
      "Файл 0 уже обработан\n",
      "Файл 1 уже обработан\n",
      "Файл 2 уже обработан\n"
     ]
    }
   ],
   "source": [
    "def execute_query(qquery, book_id):\n",
    "    \n",
    "    \"\"\"Работа с запросами\"\"\"\n",
    "    \n",
    "    sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\",agent = ua.random)\n",
    "    sparql.setQuery(qquery.format(book_id = book_id))\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    results = sparql.query().convert()\n",
    "    return results['results']['bindings']\n",
    "\n",
    "def entity_from_sparql(results):\n",
    "    \n",
    "    \"\"\"Обработка исходящих предикатов\"\"\"\n",
    "    \n",
    "    entity = {}\n",
    "    for result in results:#Построчно читаем\n",
    "        wdLabel, ps_Label, ps_, wd, direction = result['wdLabel']['value'], result['ps_Label']['value'], \\\n",
    "                        result['ps_']['value'], result['wd']['value'], result['direction']['value']\n",
    "        ps = {\n",
    "            'propValueID': ps_,\n",
    "            'propValue': ps_Label\n",
    "        }\n",
    "        if wdLabel in entity:# Формируем словари для повторяющихся предикатов\n",
    "                entity[wdLabel]['content'].append(ps)\n",
    "        else:\n",
    "            entity[wdLabel] = {\n",
    "                'propID': wd,\n",
    "                'direction': direction,\n",
    "                'content': [ps]\n",
    "            }\n",
    "    return entity\n",
    "\n",
    "def entity_to_sparql(results):\n",
    "    \n",
    "    \"\"\"Обрабокта входящих предикатов\"\"\"\n",
    "    \n",
    "    entity = {}\n",
    "    for result in results:\n",
    "        obj, objLabel, wd, prop_label, bookLabel, direction = result['obj']['value'], result['objLabel']['value'], \\\n",
    "                        result['wd']['value'], result['prop_label']['value'], \\\n",
    "                        result['bookLabel']['value'], result['direction']['value']\n",
    "        ps = {\n",
    "            'propValueID': obj,\n",
    "            'propValue': objLabel\n",
    "        }\n",
    "        \n",
    "        if prop_label in entity:\n",
    "                entity[prop_label]['content'].append(ps)\n",
    "        else:\n",
    "            entity[prop_label] = {\n",
    "                'propID': wd,\n",
    "                'direction': direction,\n",
    "                'content': [ps]\n",
    "            }\n",
    "    return entity\n",
    "    \n",
    "\n",
    "def get_properties_of_book(book_id, book_name=None):\n",
    "    \n",
    "    \"\"\"Формирование датафрейма и JSON \"\"\"\n",
    "    \n",
    "    results_from = execute_query(bookquery.query_from, book_id)\n",
    "    results_to = execute_query(bookquery.query_to, book_id)\n",
    "    \n",
    "    entity_from = entity_from_sparql(results_from)\n",
    "    entity_to = entity_to_sparql(results_to)\n",
    "    \n",
    "    entity = {**entity_from, **entity_to}# Объединяем словарь \n",
    "\n",
    "    _book_name = book_name\n",
    "    \n",
    "    counter, res = {}, []\n",
    "    for k, v in entity.items(): \n",
    "        counter[k] = len(v['content'])\n",
    "    for k, v in counter.items():# Формирвоание датафрейма\n",
    "        prop = {\n",
    "            'entity_id': book_id,\n",
    "            'entity_name': _book_name,\n",
    "            'prop_name': k,\n",
    "            'prop_count':1,\n",
    "            'prop_count_value': v,\n",
    "            'prop_dir': entity[k]['direction']\n",
    "        }\n",
    "        res.append(prop)\n",
    "    return pd.DataFrame(res)\n",
    "\n",
    "def processing(v, f=0):# \n",
    "    \n",
    "    \"\"\"Обработка запроса\"\"\"\n",
    "    \n",
    "    if f > 20:# Если не удается , то ничего не возвращаем\n",
    "            return None\n",
    "    try:\n",
    "        book_id, bookLabel = v['entity'], v['label']\n",
    "        dataframe= get_properties_of_book(book_id, book_name=bookLabel)\n",
    "      \n",
    "        return  dataframe\n",
    "    except Exception as e:\n",
    "        time.sleep(3)\n",
    "        return processing(v, f + 1)\n",
    "\n",
    "\n",
    "        \n",
    "def entity_analyzer(filename,path):\n",
    "    \n",
    "    \"\"\"Создание csv-файлов\"\"\"\n",
    "    \n",
    "    curr_path = join(curr_directory,join(\"Information\",path + \"\\\\analyzing\")) # Обработчик файлов\n",
    "    Label = filename.split('\\\\')[-1].split('.')[0].split('_')[-1]# Название файла\n",
    "    file_name = filename.split('\\\\')[-1].split('.')[0].split('_')[0:3]\n",
    "    file_name = '_'.join(file_name)\n",
    "    label  = filename.split('\\\\')[-1].split('.')[0].split('_')[-2]\n",
    "    if Label not in list (map(lambda x:x.split('\\\\')[-1].split('.')[0].split('_')[-1],counter_for_files(curr_path))):\n",
    "        print(f'Файл номер {Label} начал обрабатываться {datetime.datetime.now().time()}')\n",
    "        start = time.time()\n",
    "        ids = pd.read_csv(filename, encoding='utf8',sep = ';')\n",
    "        stats_dfs = []\n",
    "        entities = []\n",
    "        res = []\n",
    "        result = []\n",
    "        for k, v in ids.iterrows():#Запросы по каждой сущности\n",
    "#             result.append(processing(v))\n",
    "            res.append(dask.delayed(processing)(v))\n",
    "        if res == []:\n",
    "            print(f'Файл {Label} пуст')\n",
    "            return None\n",
    "        result = dask.compute(*res)\n",
    "        errors_counter = 0\n",
    "        for i in range(len(result)):#Формируем датафреймы\n",
    "            try:\n",
    "                stats_dfs.append(result[i])\n",
    "            except Exception as e:\n",
    "                errors_counter+=1\n",
    "                continue\n",
    "        #Создание файла со всеми предикатами для каждой сущности        \n",
    "       \n",
    "        stats_df = pd.concat(stats_dfs)#Создание файлов\n",
    "        stats_df.sort_values(by = 'prop_count',ascending = False).to_csv(\n",
    "            curr_path + f'/analyzed_{file_name}_{Label}.csv', encoding='utf-8-sig', index=False,sep = ';')\n",
    "# Создание агрегированного по сущностям файла для предикатов\n",
    "        stats_aggregated = stats_df.groupby('prop_name').sum().sort_values(by = ['prop_count','prop_count_value'],ascending = False)\n",
    "        stats_aggregated['prop_counter'] = stats_aggregated['prop_count']/(len(ids)-errors_counter) * 100\n",
    "        stats_aggregated.to_csv(curr_path + f'/aggregated/{file_name}_aggregated_{Label}.csv', encoding='utf-8-sig',sep = ';')\n",
    "\n",
    "\n",
    "\n",
    "        end = time.time()\n",
    "        print(f'Файл {file_name}_{Label} отработал:количество необработанных файлов {errors_counter} из {len(ids)}(успех:{(len(ids)-errors_counter)*100/(len(ids))} %), заняло {(end-start)/60} минут')\n",
    "    else:\n",
    "        print(f'Файл {Label} уже обработан')\n",
    "\n",
    "\n",
    "        \n",
    "def global_analyzer(path):\n",
    "    \n",
    "    \"\"\"Главная функция обработки файлов\"\"\"\n",
    "    \n",
    "    res = []\n",
    "    curr_directory = os.getcwd()\n",
    "    \n",
    "    new_path_main = join(curr_directory,join(\"Information\",path + r\"\\analyzing\"))\n",
    "    path_for_finding = join(curr_directory,join(\"Information\",path + r\"\\data\\data_with_popularity\"))\n",
    "    try:\n",
    "        os.mkdir(new_path_main)\n",
    "    except OSError:\n",
    "        print (\"Директория %s уже создана\" % new_path_main)\n",
    "\n",
    "    new_path = join(new_path_main,'aggregated')\n",
    "    try:\n",
    "        os.mkdir(new_path)\n",
    "    except OSError:\n",
    "        print (\"Директория %s уже создана\" % new_path)\n",
    "\n",
    "    new_path = join(new_path_main,'aggregated_with_popularity')\n",
    "    try:\n",
    "        os.mkdir(new_path)\n",
    "    except OSError:\n",
    "        print (\"Директория %s уже создана\" % new_path)\n",
    "    for filename in counter_for_files(path_for_finding):\n",
    "        entity_analyzer(filename,path)\n",
    "        #res.append(dask.delayed( entity_analyzer)(filename,path))\n",
    "    #result = dask.compute(*res)\n",
    "\n",
    "\n",
    "global_analyzer(\"2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
